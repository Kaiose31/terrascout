{"cells":[{"cell_type":"markdown","metadata":{"id":"eojK1BT9MvxA"},"source":["## Setup and data loading"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kYCCoF99MvxA","executionInfo":{"status":"ok","timestamp":1731262452134,"user_tz":480,"elapsed":11274,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf  # For tf.data\n","import matplotlib.pyplot as plt\n","import keras\n","from keras import layers\n","from keras.applications import EfficientNetB3\n","\n","# IMG_SIZE is determined by EfficientNet model choice\n","IMG_SIZE = 300\n","BATCH_SIZE = 16\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import zipfile\n","with zipfile.ZipFile('/content/drive/MyDrive/insect.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/images')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wk1tpcmQnk7X","executionInfo":{"status":"ok","timestamp":1731262494187,"user_tz":480,"elapsed":42057,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}},"outputId":"edf6820c-4f9b-4b66-ffdf-60f3b253be5d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","obv=pd.read_csv('observations-497982.csv')"],"metadata":{"id":"7istHZ0iN_ND","executionInfo":{"status":"ok","timestamp":1731262494189,"user_tz":480,"elapsed":16,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df=obv[obv['iconic_taxon_name']=='Insecta']\n","len(df)"],"metadata":{"id":"9txo9WIGOrE6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731262494190,"user_tz":480,"elapsed":14,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}},"outputId":"4b7bb02f-f532-4814-9c11-553823059ed5"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["602"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import os\n","path = \"/content/images/data/insect_img/\"\n","fileList=os.listdir(path)\n","print(len(fileList))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1hT5xdfy_mr","executionInfo":{"status":"ok","timestamp":1731262494190,"user_tz":480,"elapsed":12,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}},"outputId":"b61c1b96-c2ee-4545-a908-de189239af96"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["8239\n"]}]},{"cell_type":"code","source":["import os\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","\n","# Create an empty list to store images and labels\n","images = []\n","labels = []\n","\n","# Create a Counter to count occurrences of each label (scientific_name)\n","label_counts = Counter(df['scientific_name'])\n","\n","# Get the top 100 labels with the most occurrences\n","top_100_labels = [label for label, count in label_counts.most_common(100)]\n","\n","# Loop through the CSV to read image ids and their corresponding labels\n","for index, row in df.iterrows():\n","    img_path = f'/content/images/data/insect_img/{row[\"id\"]}.jpg'  # Assuming the image is in the same folder\n","    label = row['scientific_name']\n","\n","    # Check if the image file exists and if the label is in the top 100\n","    if os.path.exists(img_path) and label in top_100_labels:\n","        # Load and preprocess the image\n","        img = load_img(img_path, target_size=(224, 224))  # Resize image to 224x224 or any preferred size\n","        img = img_to_array(img)  # Convert to array\n","        images.append(img)\n","        labels.append(label)\n","\n","# Convert lists to arrays\n","images = np.array(images)\n","labels = np.array(labels)\n","\n","# Combine images and labels into a dataset\n","data = list(zip(images, labels))"],"metadata":{"id":"9eiGksJgbT0L","executionInfo":{"status":"ok","timestamp":1731265880068,"user_tz":480,"elapsed":1380,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["train_data, test_data = train_test_split(data, test_size=0.2, random_state=10)\n","\n"],"metadata":{"id":"x_aI22eR1rui","executionInfo":{"status":"ok","timestamp":1731265895233,"user_tz":480,"elapsed":261,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Split train and test data into separate images and labels\n","train_images = np.array([item[0] for item in train_data])\n","train_labels = np.array([item[1] for item in train_data])\n","test_images = np.array([item[0] for item in test_data])\n","test_labels = np.array([item[1] for item in test_data])"],"metadata":{"id":"H9TrXPPbK5CO","executionInfo":{"status":"ok","timestamp":1731265897948,"user_tz":480,"elapsed":364,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Fit the label encoder on both the train and test labels\n","all_labels = np.concatenate([train_labels, test_labels])\n","\n","label_encoder = LabelEncoder()\n","label_encoder.fit(all_labels)  # Fit on all available labels\n","\n","# Transform both the train and test labels\n","train_labels = label_encoder.transform(train_labels)\n","test_labels = label_encoder.transform(test_labels)\n","\n","# Now one-hot encode the labels\n","train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=np.unique(all_labels).size)\n","test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=np.unique(all_labels).size)\n","\n","\n","# Convert to a tf.data.Dataset\n","ds_train = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n","ds_test = tf.data.Dataset.from_tensor_slices((test_images, test_labels))"],"metadata":{"id":"2P-1kpLi4EAO","executionInfo":{"status":"ok","timestamp":1731265903493,"user_tz":480,"elapsed":433,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["NUM_CLASSES= np.unique(labels).size"],"metadata":{"id":"ir77o6ovruWi","executionInfo":{"status":"ok","timestamp":1731265907309,"user_tz":480,"elapsed":148,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NzCEzeuoMvxC"},"source":["When the dataset include images with various size, we need to resize them into a\n","shared size. The Stanford Dogs dataset includes only images at least 200x200\n","pixels in size. Here we resize the images to the input size needed for EfficientNet."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"VSoGMHlAMvxC","executionInfo":{"status":"ok","timestamp":1731265910640,"user_tz":480,"elapsed":455,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"outputs":[],"source":["size = (IMG_SIZE, IMG_SIZE)\n","ds_train = ds_train.map(lambda image, label: (tf.image.resize(image, size), label))\n","ds_test = ds_test.map(lambda image, label: (tf.image.resize(image, size), label))"]},{"cell_type":"markdown","metadata":{"id":"yjIVqZujMvxH"},"source":["### Prepare inputs\n","\n","Once we verify the input data and augmentation are working correctly,\n","we prepare dataset for training. The input data are resized to uniform\n","`IMG_SIZE`. The labels are put into one-hot\n","(a.k.a. categorical) encoding. The dataset is batched.\n","\n","Note: `prefetch` and `AUTOTUNE` may in some situation improve\n","performance, but depends on environment and the specific dataset used.\n","See this [guide](https://www.tensorflow.org/guide/data_performance)\n","for more information on data pipeline performance."]},{"cell_type":"code","source":["# # Define augmentation layers\n","# img_augmentation_layers = [\n","#     layers.RandomRotation(factor=0.15),\n","#     layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n","#     layers.RandomFlip(),\n","#     layers.RandomContrast(factor=0.1),\n","# ]\n","\n","# def img_augmentation(images):\n","#     for layer in img_augmentation_layers:\n","#         images = layer(images)\n","#     return images\n","\n","# # Apply augmentation to the train dataset\n","# ds_train = ds_train.map(lambda image, label: (img_augmentation(image), label))  # Apply augmentation\n"],"metadata":{"id":"EslbKldlDVDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":24,"metadata":{"id":"7mVmP_W4MvxH","executionInfo":{"status":"ok","timestamp":1731265922327,"user_tz":480,"elapsed":771,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"outputs":[],"source":["ds_train = ds_train.shuffle(buffer_size=100).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n","ds_test = ds_test.batch(32).prefetch(tf.data.experimental.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"vk1csQXoMvxJ"},"source":["## Transfer learning from pre-trained weights\n","\n","Here we initialize the model with pre-trained ImageNet weights,\n","and we fine-tune it on our own dataset."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"XcnEWdk9MvxJ","executionInfo":{"status":"ok","timestamp":1731265925318,"user_tz":480,"elapsed":205,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}}},"outputs":[],"source":["\n","def build_model(num_classes):\n","    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n","    model = EfficientNetB3(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n","\n","    # Freeze the pretrained weights\n","    model.trainable = False\n","\n","    # Rebuild top\n","    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n","    x = layers.BatchNormalization()(x)\n","\n","    top_dropout_rate = 0.2\n","    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n","    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n","\n","    # Compile\n","    model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n","    optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n","    model.compile(\n","        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n","    )\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"_-Fyh4tFMvxJ"},"source":["The first step to transfer learning is to freeze all layers and train only the top\n","layers. For this step, a relatively large learning rate (1e-2) can be used.\n","Note that validation accuracy and loss will usually be better than training\n","accuracy and loss. This is because the regularization is strong, which only\n","suppresses training-time metrics.\n","\n","Note that the convergence may take up to 50 epochs depending on choice of learning rate.\n","If image augmentation layers were not\n","applied, the validation accuracy may only reach ~60%."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"hYkLuEMNMvxK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731267135353,"user_tz":480,"elapsed":1206730,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}},"outputId":"92ca2d2b-c5ef-4018-d9bd-8c0199aa5cc6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 12s/step - accuracy: 0.0776 - loss: 5.9435 - val_accuracy: 0.1951 - val_loss: 3.8729\n","Epoch 2/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 13s/step - accuracy: 0.7979 - loss: 0.8516 - val_accuracy: 0.1951 - val_loss: 3.7997\n","Epoch 3/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 13s/step - accuracy: 0.8838 - loss: 0.4478 - val_accuracy: 0.2195 - val_loss: 3.9769\n","Epoch 4/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 13s/step - accuracy: 0.9583 - loss: 0.1912 - val_accuracy: 0.2317 - val_loss: 3.8991\n","Epoch 5/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 14s/step - accuracy: 0.9519 - loss: 0.1520 - val_accuracy: 0.2805 - val_loss: 3.6930\n","Epoch 6/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 13s/step - accuracy: 0.9746 - loss: 0.0817 - val_accuracy: 0.2195 - val_loss: 3.8399\n","Epoch 7/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 13s/step - accuracy: 0.9818 - loss: 0.0739 - val_accuracy: 0.2561 - val_loss: 3.9329\n","Epoch 8/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 12s/step - accuracy: 0.9850 - loss: 0.0574 - val_accuracy: 0.2317 - val_loss: 3.9349\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x791d2ef7fa90>"]},"metadata":{},"execution_count":26}],"source":["model = build_model(num_classes=NUM_CLASSES)\n","\n","epochs = 15\n","\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=3, restore_best_weights=True\n",")\n","\n","model.fit(ds_train, epochs=epochs, validation_data=ds_test, callbacks=[early_stopping])"]},{"cell_type":"markdown","metadata":{"id":"ALaSXGAKMvxK"},"source":["The second step is to unfreeze a number of layers and fit the model using smaller\n","learning rate. In this example we show unfreezing all layers, but depending on\n","specific dataset it may be desireble to only unfreeze a fraction of all layers.\n","\n","When the feature extraction with\n","pretrained model works good enough, this step would give a very limited gain on\n","validation accuracy. In our case we only see a small improvement,\n","as ImageNet pretraining already exposed the model to a good amount of dogs.\n","\n","On the other hand, when we use pretrained weights on a dataset that is more different\n","from ImageNet, this fine-tuning step can be crucial as the feature extractor also\n","needs to be adjusted by a considerable amount. Such a situation can be demonstrated\n","if choosing CIFAR-100 dataset instead, where fine-tuning boosts validation accuracy\n","by about 10% to pass 80% on `EfficientNetB0`.\n","\n","A side note on freezing/unfreezing models: setting `trainable` of a `Model` will\n","simultaneously set all layers belonging to the `Model` to the same `trainable`\n","attribute. Each layer is trainable only if both the layer itself and the model\n","containing it are trainable. Hence when we need to partially freeze/unfreeze\n","a model, we need to make sure the `trainable` attribute of the model is set\n","to `True`."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"f33MNvs4MvxK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731264402783,"user_tz":480,"elapsed":1044432,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}},"outputId":"f6cc8bce-1c3a-4c4f-9535-2d0ba222ec75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 7s/step - accuracy: 0.0109 - loss: 10.4128 - val_accuracy: 0.0413 - val_loss: 5.8804\n","Epoch 2/5\n","\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 6s/step - accuracy: 0.0036 - loss: 9.4311 - val_accuracy: 0.0413 - val_loss: 5.8854\n","Epoch 3/5\n","\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 6s/step - accuracy: 0.0069 - loss: 8.9002 - val_accuracy: 0.0331 - val_loss: 5.8919\n","Epoch 4/5\n","\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 6s/step - accuracy: 0.0102 - loss: 8.1637 - val_accuracy: 0.0165 - val_loss: 5.8869\n","Epoch 5/5\n","\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 6s/step - accuracy: 0.0161 - loss: 8.1481 - val_accuracy: 0.0083 - val_loss: 5.8728\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x791d31c1c1c0>"]},"metadata":{},"execution_count":9}],"source":["\n","def unfreeze_model(model):\n","    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n","    for layer in model.layers[-20:]:\n","        if not isinstance(layer, layers.BatchNormalization):\n","            layer.trainable = True\n","\n","    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n","    model.compile(\n","        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n","    )\n","\n","\n","unfreeze_model(model)\n","\n","epochs = 5 # @param {type: \"slider\", min:4, max:10}\n","model.fit(ds_train, epochs=epochs, validation_data=ds_test)\n","# plot_hist(hist)"]},{"cell_type":"code","source":["# Evaluate on test dataset\n","test_loss, test_accuracy = model.evaluate(ds_test)\n","\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-h2DcP56SGB","executionInfo":{"status":"ok","timestamp":1731267303362,"user_tz":480,"elapsed":41227,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}},"outputId":"75ff446e-362a-4fa7-f4ee-f07bb30edba7"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8s/step - accuracy: 0.2809 - loss: 3.6567\n","Test Accuracy: 28.05%\n"]}]},{"cell_type":"code","source":["model.save('model.h5')\n","from google.colab import files\n","files.download(\"model.h5\")\n","import pickle\n","with open('Insect_encoder.pkl', 'wb') as f:\n","    pickle.dump(label_encoder, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"urxh2HdoE68g","executionInfo":{"status":"ok","timestamp":1731267410900,"user_tz":480,"elapsed":842,"user":{"displayName":"Aryan Vats","userId":"11186676365311780370"}},"outputId":"5cc3a5cc-63a3-4064-aaa2-358706cf81da"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_8aacd8ea-99c6-4999-8875-aa5343c1603e\", \"model.h5\", 46201032)"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"O1WuhDj4MvxK"},"source":["### Tips for fine tuning EfficientNet\n","\n","On unfreezing layers:\n","\n","- The `BatchNormalization` layers need to be kept frozen\n","([more details](https://keras.io/guides/transfer_learning/)).\n","If they are also turned to trainable, the\n","first epoch after unfreezing will significantly reduce accuracy.\n","- In some cases it may be beneficial to open up only a portion of layers instead of\n","unfreezing all. This will make fine tuning much faster when going to larger models like\n","B7.\n","- Each block needs to be all turned on or off. This is because the architecture includes\n","a shortcut from the first layer to the last layer for each block. Not respecting blocks\n","also significantly harms the final performance.\n","\n","Some other tips for utilizing EfficientNet:\n","\n","- Larger variants of EfficientNet do not guarantee improved performance, especially for\n","tasks with less data or fewer classes. In such a case, the larger variant of EfficientNet\n","chosen, the harder it is to tune hyperparameters.\n","- EMA (Exponential Moving Average) is very helpful in training EfficientNet from scratch,\n","but not so much for transfer learning.\n","- Do not use the RMSprop setup as in the original paper for transfer learning. The\n","momentum and learning rate are too high for transfer learning. It will easily corrupt the\n","pretrained weight and blow up the loss. A quick check is to see if loss (as categorical\n","cross entropy) is getting significantly larger than log(NUM_CLASSES) after the same\n","epoch. If so, the initial learning rate/momentum is too high.\n","- Smaller batch size benefit validation accuracy, possibly due to effectively providing\n","regularization."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}